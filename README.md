including but not limited to face reenactment, talking head generation
(will be completed soon)

# Institutions
### NVIDIA, Netease Fuxi, Tencent AI, SenseTime, Samsung, Xiaobing, Baidu, Huawei

# Person (as far as i know)
#### [Liu Ziwei](https://liuziwei7.github.io/publications.html)
#### []

# Paper

### General keywords: audio-driven, stylegan, transformer, 3D, emotion, video, FOMM (including flow), landmark (not limited to keypoints), wav2lip, Nerf, 3DMM, diffusion, flow
### Other keyword: pose-controllable, edge map, depth map, 3D FOMM (indicating [this](https://arxiv.org/abs/2011.15126))

## 2023
### \[] GENEFACE: TOWARDS HIGH-FIDELITY AUDIO DRIVEN 3D TALKING FACE SYNTHESIS GENERALIZABLE TO OUT-OF-DOMAIN AUDIO
keywords: audio-driven, Nerf, landmark
[project page](https://geneface.github.io/), 
[paper](https://openreview.net/forum?id=YfwMIDhPccD), 
code [Not found]

### \[] 
keywords:
project page, 
paper, 
code

## 2022
### \[] SPACE: Speech-driven Portrait Animation with Controllable Expression
keywords:
[project page](https://deepimagination.cc/SPACEx/), 
[paper](https://arxiv.org/pdf/2211.09809), 
code [Not found]

### \[] StyleMask: Disentangling the Style Space of StyleGAN2 for Neural ace Reenactment
keywords:
project page, 
paper, 
code

### \[] 
keywords:
project page, 
paper, 
code

### \[] MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation
keywords: flow, landmark (interesting), 
[project page](https://meta-portrait.github.io), 
[paper](https://arxiv.org/abs/2212.08062), 
[code](https://github.com/Meta-Portrait/MetaPortrait)

### \[arxiv] High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors
keywords: Nerf, 3DMM, audio-driven, 3D-GAN
project page [Not found], 
[paper](https://arxiv.org/abs/2211.15064), 
code [Not found]

### \[arxiv] StyleFaceV: Face Video Generation via Decomposing and Recomposing Pretrained StyleGAN3
keywords: stylegan, video, landmark
[project page](http://haonanqiu.com/projects/StyleFaceV.html), 
[paper](https://arxiv.org/abs/2208.07862), 
[code](https://github.com/arthur-qiu/StyleFaceV)

### \[arxiv] SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation
keywords: 3D, emotion, video, audio-driven, 3DMM
[project page](https://sadtalker.github.io), 
[paper](https://arxiv.org/abs/2211.12194), 
[code](https://github.com/Winfredy/SadTalker)

### \[SIGGRAPH] VideoReTalking: Audio-based Lip Synchronization for Talking HeadVideo Editing In the Wild
keywords: audio-driven, video, wav2lip, transformer, emotion
[project page](https://vinthony.github.io/video-retalking/), 
[paper](https://arxiv.org/abs/2211.14758), 
[code](https://github.com/vinthony/video-retalking/)

### \[SIGGRAPH] Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers
keywords: Transformer, audio-driven, wav2lip, video
[project page](https://hangz-nju-cuhk.github.io/projects/AV-CAT), 
[paper](https://arxiv.org/abs/2212.04970), 
code [Not found]

### \[CVPR] Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation
keywords: audio-driven, pose-controllable, video
[project page](https://hangz-nju-cuhk.github.io/projects/PC-AVS), 
[paper](https://arxiv.org/abs/2104.11116), 
[code](https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS)

### \[CVPR] Depth-Aware Generative Adversarial Network for Talking Head Video Generation
keywords: FOMM, depth map
[project page](https://harlanhong.github.io/publications/dagan.html), 
[paper](https://arxiv.org/abs/2203.06605), 
[code](https://github.com/harlanhong/CVPR2022-DaGAN)

### \[arxiv] One-Shot Face Reenactment on Megapixels
keywords: stylegan, 3DMM, landmark
project page [Not found], 
[paper](https://arxiv.org/pdf/2205.13368), 
code [Not found]

### \[ECCV] StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN
keywords: stylegan, flow, audio-driven, 3DMM
[project page](http://feiiyin.github.io/StyleHEAT/), 
[paper](https://arxiv.org/abs/2203.04036), 
[code](https://github.com/FeiiYin/StyleHEAT/)

### \[ECCV] Face2Faceρ: Real-Time High-Resolution One-Shot Face Reenactment
keywords: keypoint (interesting), 3DMM, flow
project page [Not found], 
[paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136730055.pdf), 
[code](https://github.com/NetEase-GameAI/Face2FaceRHO)

### \[CVPR] Expressive Talking Head Generation with Granular Audio-Visual Control
keywords:
project page, 
[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expressive_Talking_Head_Generation_With_Granular_Audio-Visual_Control_CVPR_2022_paper.pdf
), 
code

### \[ACM MM] MegaPortraits: One-shot Megapixel Neural Head Avatars
keywords: 3DMM, 3D FOMM, 
[project page](https://samsunglabs.github.io/MegaPortraits/), 
[paper](https://arxiv.org/abs/2207.07621), 
code [None]

### \[CVPR] Thin-Plate Spline Motion Model for Image Animation
keywords:
project page [None], 
[paper](https://arxiv.org/abs/2203.14367), 
[code](https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model)

### \[] Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation
keywords:
project page, 
paper, 
code

## 2021
### \[ICCV] PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering
keywords:
[project page](https://renyurui.github.io/PIRender_web/), 
[paper](https://arxiv.org/abs/2109.08379), 
[code](https://github.com/RenYurui/PIRender)

### \[CVPR] Audio-Driven Emotional Video Portraits
keywords: landmark, audio-driven, 3DMM, edge map，video
[project page](https://jixinya.github.io/projects/evp/), 
[paper](https://arxiv.org/abs/2104.07452), 
[code](https://github.com/jixinya/EVP)

### \[ICCV] HeadGAN: One-shot Neural Head Synthesis and Editing
keywords: 3DMM, audio-driven, flow
[project page](https://michaildoukas.github.io/HeadGAN/), 
[paper](https://arxiv.org/abs/2012.08261), 
code [None]

### \[ICCV] FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning
keywords: audio-driven, 3DMM
[project page](https://personal.utdallas.edu/~xguo/), 
[paper](https://arxiv.org/abs/2108.07938), 
[code](https://github.com/zhangchenxu528/FACIAL)

### \[] One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing
keywords:
[project page](https://nvlabs.github.io/face-vid2vid), 
[paper](https://arxiv.org/abs/2011.15126), 
[code](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis) [unoffical]

### \[SIGGRAPH] Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation
keywords:
project page, 
[paper](https://arxiv.org/abs/2109.10595), 
code

### \[] 
keywords:
project page, 
paper, 
code

### \[] 
keywords:
project page, 
paper, 
code

### \[IJCAI] Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion
keywords: audio-driven, FOMM
project page [Not found], 
[paper](https://www.ijcai.org/proceedings/2021/0152.pdf), 
[code](https://github.com/wangsuzhen/Audio2Head)

## 2020
### \[ACM MM]\[Wav2Lip] A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild
keywords: video, wav2lip, audio-driven
[project page](http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/), 
[paper](https://arxiv.org/abs/2008.10010), 
[code](https://github.com/Rudrabha/Wav2Lip)

### \[ISMAR] Photorealistic Audio-driven Video Portraits
keywords: audio-driven, 3DMM, video
[project page](https://richardt.name/publications/audio-dvp/), 
[paper](http://miaowang.me/papers/advp_authors.pdf), 
[code](https://github.com/xinwen-cs/AudioDVP)
